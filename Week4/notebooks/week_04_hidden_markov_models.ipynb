{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weeks 3-4: Hidden Markov Models (HMMs)\n",
    "\n",
    "**Objective:** Understand the structure of a Hidden Markov Model, its components, and how to solve the \"decoding\" problem (finding the most likely sequence of hidden states) using the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build Intuition\n",
    "\n",
    "In Week 3, we modeled weather using a Markov Chain where we could directly see the states (`Sunny`, `Rainy`). Now, imagine you are a detective locked in a windowless basement. You can't see the weather outside, so the state (`Sunny` or `Rainy`) is **hidden** from you.\n",
    "\n",
    "However, each day your partner comes downstairs. You can observe whether your partner is carrying an **umbrella**. This observation gives you a clue about the hidden weather state.\n",
    "\n",
    "- If it's `Rainy` (hidden state), it's highly likely your partner has an `Umbrella` (observation).\n",
    "- If it's `Sunny` (hidden state), it's unlikely your partner has an `Umbrella`.\n",
    "\n",
    "A **Hidden Markov Model (HMM)** is perfect for this scenario. It models a system with an underlying Markov process that we cannot observe, but we can see outputs or signals that depend on the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understand the Core Idea\n",
    "\n",
    "An HMM has two layers:\n",
    "1.  **The Hidden Layer:** A standard Markov Chain with states and transition probabilities. We can't see this layer.\n",
    "2.  **The Observation Layer:** A set of possible observations that are probabilistically linked to the hidden states.\n",
    "\n",
    "To fully define an HMM, we need three key components:\n",
    "\n",
    "1.  **Transition Probabilities (A):** The probability of moving from one *hidden state* to another. This is the same as the Transition Probability Matrix (TPM) from a regular Markov Chain.\n",
    "    - `A[i, j] = P(hidden_state_j at t+1 | hidden_state_i at t)`\n",
    "\n",
    "2.  **Emission Probabilities (B):** The probability of seeing a particular *observation* given that the system is in a specific *hidden state*.\n",
    "    - `B[i, k] = P(observation_k | hidden_state_i)`\n",
    "\n",
    "3.  **Initial State Probabilities (π):** The probability of the hidden process starting in each possible state at time `t=0`.\n",
    "    - `π[i] = P(hidden_state_i at t=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Learn the Definitions and Formulas\n",
    "\n",
    "An HMM is formally defined by the tuple \\(\\lambda = (A, B, \\pi)\\).\n",
    "\n",
    "For our detective example:\n",
    "- **Hidden States:** `S = {Sunny, Rainy}`\n",
    "- **Observations:** `O = {Umbrella, No Umbrella}`\n",
    "\n",
    "**1. Transition Matrix (A):** (Same as last week)\n",
    "$$ A = \\begin{pmatrix} P(S \\to S) & P(S \\to R) \\\\ P(R \\to S) & P(R \\to R) \\end{pmatrix} = \\begin{pmatrix} 0.8 & 0.2 \\\\ 0.4 & 0.6 \\end{pmatrix} $$\n",
    "\n",
    "**2. Emission Matrix (B):**\n",
    "$$ B = \\begin{pmatrix} P(Umbrella|S) & P(No Umbrella|S) \\\\ P(Umbrella|R) & P(No Umbrella|R) \\end{pmatrix} = \\begin{pmatrix} 0.1 & 0.9 \\\\ 0.7 & 0.3 \\end{pmatrix} $$\n",
    "\n",
    "**3. Initial Probabilities (π):** (Let's assume it's equally likely to be sunny or rainy on day 1)\n",
    "$$ \\pi = \\begin{pmatrix} P(S) & P(R) \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\end{pmatrix} $$\n",
    "\n",
    "### The Three Fundamental Problems of HMMs\n",
    "1.  **Likelihood:** Given an HMM \\(\\lambda\\) and a sequence of observations, what is the probability of observing that sequence? (Solved by the **Forward Algorithm**).\n",
    "2.  **Decoding:** Given an HMM \\(\\lambda\\) and a sequence of observations, what is the most likely sequence of *hidden states* that produced these observations? (Solved by the **Viterbi Algorithm**).\n",
    "3.  **Learning:** Given a sequence of observations, what are the HMM parameters \\(\\lambda = (A, B, \\pi)\\) that best explain the data? (Solved by the **Baum-Welch Algorithm**).\n",
    "\n",
    "In this notebook, we will focus on the **Decoding** problem, as it is highly intuitive and useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply and Practice (The Viterbi Algorithm)\n",
    "\n",
    "**Problem:** Suppose over 3 days, you observe your partner's behavior as: `[Umbrella, No Umbrella, Umbrella]`.\n",
    "\n",
    "What was the most likely sequence of weather for those three days?\n",
    "\n",
    "The Viterbi algorithm finds this most probable path using dynamic programming. It builds a table of probabilities, moving one step at a time, and at each step, it only keeps the most likely path to get to that state.\n",
    "\n",
    "Let's implement it from scratch to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the HMM parameters\n",
    "# States: 0=Sunny, 1=Rainy\n",
    "states = ['Sunny', 'Rainy']\n",
    "pi = np.array([0.5, 0.5])\n",
    "A = np.array([\n",
    "    [0.8, 0.2],  # Sunny -> Sunny, Sunny -> Rainy\n",
    "    [0.4, 0.6]   # Rainy -> Sunny, Rainy -> Rainy\n",
    "])\n",
    "\n",
    "# Observations: 0=Umbrella, 1=No Umbrella\n",
    "observations = ['Umbrella', 'No Umbrella']\n",
    "B = np.array([\n",
    "    [0.1, 0.9],  # P(Umbrella|Sunny), P(No Umbrella|Sunny)\n",
    "    [0.7, 0.3]   # P(Umbrella|Rainy), P(No Umbrella|Rainy)\n",
    "])\n",
    "\n",
    "# The sequence of observations we saw\n",
    "# 0 = Umbrella, 1 = No Umbrella, 0 = Umbrella\n",
    "obs_seq = [0, 1, 0]\n",
    "\n",
    "print(\"HMM Parameters:\")\n",
    "print(\"Initial Probs (pi):\", pi)\n",
    "print(\"Transition Matrix (A):\\n\", A)\n",
    "print(\"Emission Matrix (B):\\n\", B)\n",
    "print(\"\\nObserved Sequence:\", [observations[i] for i in obs_seq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(obs_seq, states, pi, A, B):\n",
    "    \"\"\"Finds the most likely sequence of hidden states (Viterbi algorithm).\"\"\"\n",
    "    num_states = A.shape[0]\n",
    "    T = len(obs_seq)\n",
    "    \n",
    "    # Viterbi table (stores max probabilities)\n",
    "    viterbi_table = np.zeros((num_states, T))\n",
    "    # Backpointer table (stores the path)\n",
    "    backpointer = np.zeros((num_states, T), dtype=int)\n",
    "    \n",
    "    # --- 1. Initialization Step ---\n",
    "    # Probability of starting in state i AND seeing the first observation\n",
    "    first_obs = obs_seq[0]\n",
    "    viterbi_table[:, 0] = pi * B[:, first_obs]\n",
    "    \n",
    "    # --- 2. Recursion Step ---\n",
    "    for t in range(1, T):\n",
    "        obs = obs_seq[t]\n",
    "        for s in range(num_states):\n",
    "            # Probability of transitioning from any previous state to current state s\n",
    "            prev_probs = viterbi_table[:, t-1] * A[:, s]\n",
    "            \n",
    "            # Find the max probability and the state that produced it\n",
    "            max_prob = np.max(prev_probs)\n",
    "            best_prev_state = np.argmax(prev_probs)\n",
    "            \n",
    "            # Store the probability and backpointer\n",
    "            # Max prob of path so far * prob of emitting current observation from state s\n",
    "            viterbi_table[s, t] = max_prob * B[s, obs]\n",
    "            backpointer[s, t] = best_prev_state\n",
    "            \n",
    "    # --- 3. Termination Step ---\n",
    "    best_path_prob = np.max(viterbi_table[:, -1])\n",
    "    last_state = np.argmax(viterbi_table[:, -1])\n",
    "    \n",
    "    # --- 4. Path Backtracking ---\n",
    "    best_path = [last_state]\n",
    "    for t in range(T - 1, 0, -1):\n",
    "        last_state = backpointer[last_state, t]\n",
    "        best_path.insert(0, last_state)\n",
    "        \n",
    "    return best_path, best_path_prob, viterbi_table\n",
    "\n",
    "# Run the Viterbi algorithm\n",
    "best_path_indices, prob, v_table = viterbi(obs_seq, states, pi, A, B)\n",
    "\n",
    "best_path_names = [states[i] for i in best_path_indices]\n",
    "\n",
    "print(\"--- Viterbi Algorithm Results ---\")\n",
    "print(\"Viterbi Table (log probabilities are often used in practice to avoid underflow):\\n\", v_table)\n",
    "print(f\"\\nProbability of the best path: {prob:.6f}\")\n",
    "print(f\"Most likely sequence of hidden states: {best_path_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "The algorithm suggests that the most likely weather sequence to produce the observations `[Umbrella, No Umbrella, Umbrella]` is `['Rainy', 'Sunny', 'Rainy']`.\n",
    "\n",
    "This makes intuitive sense:\n",
    "- **Day 1 (Umbrella):** Likely `Rainy`.\n",
    "- **Day 2 (No Umbrella):** The model has to decide. Is it more likely that it stayed `Rainy` and our partner just forgot their umbrella (prob 0.3), or that the weather switched to `Sunny` (transition prob 0.4) and they correctly left the umbrella at home (emission prob 0.9)? The path through `Sunny` is more probable.\n",
    "- **Day 3 (Umbrella):** The weather likely switched back to `Rainy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Library (`hmmlearn`)\n",
    "\n",
    "Implementing Viterbi from scratch is great for learning, but for real applications, specialized libraries are more robust and optimized. Let's verify our result using `hmmlearn`.\n",
    "\n",
    "First, you might need to install it: `pip install hmmlearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "\n",
    "# Create an HMM model\n",
    "# 'c' for categorical observations\n",
    "model = hmm.CategoricalHMM(n_components=len(states), n_features=len(observations))\n",
    "\n",
    "# Set the parameters we defined manually\n",
    "model.startprob_ = pi\n",
    "model.transmat_ = A\n",
    "model.emissionprob_ = B\n",
    "\n",
    "# The observation sequence needs to be a 2D array of shape (n_samples, 1)\n",
    "obs_seq_2d = np.array(obs_seq).reshape(-1, 1)\n",
    "\n",
    "# Use the 'decode' method, which implements the Viterbi algorithm\n",
    "log_prob, path_indices = model.decode(obs_seq_2d)\n",
    "\n",
    "path_names = [states[i] for i in path_indices]\n",
    "\n",
    "print(\"--- hmmlearn Library Results ---\")\n",
    "print(f\"Log Probability of the best path: {log_prob:.6f}\")\n",
    "print(f\"Most likely sequence of hidden states: {path_names}\")\n",
    "\n",
    "# Compare with our manual calculation\n",
    "print(f\"\\nOur manual probability: {prob:.6f}\")\n",
    "print(f\"hmmlearn log probability converted to normal probability: {np.exp(log_prob):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results match perfectly! This confirms our understanding and implementation of the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "In this notebook, we've made the leap from simple Markov Chains to Hidden Markov Models:\n",
    "1.  **HMMs** consist of a hidden Markov process and a set of observable emissions.\n",
    "2.  They are defined by **transition (A)**, **emission (B)**, and **initial (π)** probabilities.\n",
    "3.  The **Viterbi algorithm** is a powerful tool for decoding the most likely sequence of hidden states given a sequence of observations.\n",
    "4.  Libraries like `hmmlearn` provide efficient, production-ready implementations of HMM algorithms.\n",
    "\n",
    "HMMs are used in many fields, including speech recognition (mapping audio signals to words), bioinformatics (finding genes in DNA sequences), and finance (modeling market regimes).\n",
    "\n",
    "In **Week 5**, we will explore **Branching Processes**, which are used to model population growth, the spread of diseases, or chain reactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}